{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64975c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7484be2",
   "metadata": {},
   "source": [
    "# LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ff9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel(r\"C:\\Users\\drang\\crawl\\New folder\\Inputnlp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169b3453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"URL_ID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41178e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\drang\\\\crawl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39751db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: './crawl'\n",
      "C:\\Users\\drang\\crawl\n"
     ]
    }
   ],
   "source": [
    "cd ./crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411ee74",
   "metadata": {},
   "source": [
    "# SCRAPING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ab95d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_6284\\2392313004.py:14: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0 completed\n",
      "file 1 completed\n",
      "file 2 completed\n",
      "file 3 completed\n",
      "file 4 completed\n",
      "file 5 completed\n",
      "file 6 completed\n",
      "https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/, 7 not completed\n",
      "file 7 completed\n",
      "file 8 completed\n",
      "file 9 completed\n",
      "file 10 completed\n",
      "file 11 completed\n",
      "file 12 completed\n",
      "file 13 completed\n",
      "file 14 completed\n",
      "file 15 completed\n",
      "file 16 completed\n",
      "file 17 completed\n",
      "file 18 completed\n",
      "file 19 completed\n",
      "https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/, 20 not completed\n",
      "file 20 completed\n",
      "file 21 completed\n",
      "file 22 completed\n",
      "file 23 completed\n",
      "file 24 completed\n",
      "file 25 completed\n",
      "file 26 completed\n",
      "file 27 completed\n",
      "file 28 completed\n",
      "file 29 completed\n",
      "file 30 completed\n",
      "file 31 completed\n",
      "file 32 completed\n",
      "file 33 completed\n",
      "file 34 completed\n",
      "file 35 completed\n",
      "file 36 completed\n",
      "file 37 completed\n",
      "file 38 completed\n",
      "file 39 completed\n",
      "file 40 completed\n",
      "file 41 completed\n",
      "file 42 completed\n",
      "file 43 completed\n",
      "file 44 completed\n",
      "file 45 completed\n",
      "file 46 completed\n",
      "file 47 completed\n",
      "file 48 completed\n",
      "file 49 completed\n",
      "file 50 completed\n",
      "file 51 completed\n",
      "file 52 completed\n",
      "file 53 completed\n",
      "file 54 completed\n",
      "file 55 completed\n",
      "file 56 completed\n",
      "file 57 completed\n",
      "file 58 completed\n",
      "file 59 completed\n",
      "file 60 completed\n",
      "file 61 completed\n",
      "file 62 completed\n",
      "file 63 completed\n",
      "file 64 completed\n",
      "file 65 completed\n",
      "file 66 completed\n",
      "file 67 completed\n",
      "file 68 completed\n",
      "file 69 completed\n",
      "file 70 completed\n",
      "file 71 completed\n",
      "file 72 completed\n",
      "file 73 completed\n",
      "file 74 completed\n",
      "file 75 completed\n",
      "file 76 completed\n",
      "file 77 completed\n",
      "file 78 completed\n",
      "file 79 completed\n",
      "file 80 completed\n",
      "file 81 completed\n",
      "file 82 completed\n",
      "file 83 completed\n",
      "file 84 completed\n",
      "file 85 completed\n",
      "file 86 completed\n",
      "file 87 completed\n",
      "file 88 completed\n",
      "file 89 completed\n",
      "file 90 completed\n",
      "file 91 completed\n",
      "file 92 completed\n",
      "file 93 completed\n",
      "file 94 completed\n",
      "file 95 completed\n",
      "file 96 completed\n",
      "file 97 completed\n",
      "file 98 completed\n",
      "file 99 completed\n",
      "file 100 completed\n",
      "file 101 completed\n",
      "file 102 completed\n",
      "file 103 completed\n",
      "file 104 completed\n",
      "file 105 completed\n",
      "file 106 completed\n",
      "https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/, 107 not completed\n",
      "file 107 completed\n",
      "file 108 completed\n",
      "file 109 completed\n",
      "file 110 completed\n",
      "file 111 completed\n",
      "file 112 completed\n",
      "file 113 completed\n"
     ]
    }
   ],
   "source": [
    "# extracting and saving text file\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import os\n",
    "\n",
    "DRIVER_PATH = r\"C:\\Users\\drang\\Downloads\\chromedriver_110\\chromedriver.exe\"\n",
    "for i in range(df.shape[0]):    \n",
    "    #[144,57,44] missing url_ids, index  [107,20,7]\n",
    "    options = Options()\n",
    "    #options.add_argument(\"--window-size=1920,1200\")\n",
    "    options.add_argument(\"--headless\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "    driver.get(df.iloc[i]['URL'])\n",
    "    #if \"404\" in driver.page_source:\n",
    "    response = requests.get(df.iloc[i]['URL'])\n",
    "    if response.status_code == 200:\n",
    "        #print(\"Page loaded successfully!\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"{df.iloc[i]['URL']}, {i} not completed\")\n",
    "    \n",
    "        \n",
    "        \n",
    "        #h1 = driver.find_element(By.CSS_SELECTOR, 'p' )#.find_element(By.ID, \"1\")\n",
    "    for element in driver.find_elements(By.XPATH, '//div[@class=\"td-post-content\"]/*'): #By.CSS_SELECTOR, 'p'):\n",
    "                a = element.text\n",
    "                #'//div[@class=\"td-post-content\"]/*')\n",
    "            \n",
    "                # pat = df.iloc[i]['URL'].replace(\"/\", \"_\").replace(\":\",\"{\") + \".txt\"\n",
    "                pat = str(df.iloc[i]['URL_ID']) + \".txt\"\n",
    "                #print(a)\n",
    "                try:\n",
    "                    with open(pat, 'a') as f:\n",
    "                        f.write('\\n')\n",
    "                        for line in a:\n",
    "                            f.write(line)\n",
    "                            #print(\"a\")\n",
    "                            #f.close()\n",
    "\n",
    "                except:\n",
    "                    with open(pat, 'w') as f:\n",
    "                        for line in a:\n",
    "                            f.write(line)\n",
    "                            f.close()\n",
    "                            #print(\"w\")\n",
    "\n",
    "    \n",
    "    \n",
    "    if os.path.exists(pat):\n",
    "            print(f\"file {i} completed\")\n",
    "    \n",
    "            \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91dc83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3        40  https://insights.blackcoffer.com/will-machine-...\n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110     147  https://insights.blackcoffer.com/the-future-of...\n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
       "112     149  https://insights.blackcoffer.com/business-anal...\n",
       "113     150  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c0dc1",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4ebf7",
   "metadata": {},
   "source": [
    "###  Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c03b4a6",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenizer(tfile):\n",
    "    try:\n",
    "        with open(tfile, 'r') as f:\n",
    "                text=f.read()\n",
    "    except:\n",
    "        text=tfile\n",
    "    tokenized_text=sent_tokenize(text)\n",
    "    tokenized_word=word_tokenize(text)\n",
    "    return tokenized_text, tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2961a849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['100.txt',\n",
       " '101.txt',\n",
       " '102.txt',\n",
       " '103.txt',\n",
       " '104.txt',\n",
       " '105.txt',\n",
       " '106.txt',\n",
       " '107.txt',\n",
       " '108.txt',\n",
       " '109.txt',\n",
       " '110.txt',\n",
       " '111.txt',\n",
       " '112.txt',\n",
       " '113.txt',\n",
       " '114.txt',\n",
       " '115.txt',\n",
       " '116.txt',\n",
       " '117.txt',\n",
       " '118.txt',\n",
       " '119.txt',\n",
       " '120.txt',\n",
       " '121.txt',\n",
       " '122.txt',\n",
       " '123.txt',\n",
       " '124.txt',\n",
       " '125.txt',\n",
       " '126.txt',\n",
       " '127.txt',\n",
       " '128.txt',\n",
       " '129.txt',\n",
       " '130.txt',\n",
       " '131.txt',\n",
       " '132.txt',\n",
       " '133.txt',\n",
       " '134.txt',\n",
       " '135.txt',\n",
       " '136.txt',\n",
       " '137.txt',\n",
       " '138.txt',\n",
       " '139.txt',\n",
       " '140.txt',\n",
       " '141.txt',\n",
       " '142.txt',\n",
       " '143.txt',\n",
       " '145.txt',\n",
       " '146.txt',\n",
       " '147.txt',\n",
       " '148.txt',\n",
       " '149.txt',\n",
       " '150.txt',\n",
       " '37.txt',\n",
       " '38.txt',\n",
       " '39.txt',\n",
       " '40.txt',\n",
       " '41.txt',\n",
       " '42.txt',\n",
       " '43.txt',\n",
       " '45.txt',\n",
       " '46.txt',\n",
       " '47.txt',\n",
       " '48.txt',\n",
       " '49.txt',\n",
       " '50.txt',\n",
       " '51.txt',\n",
       " '52.txt',\n",
       " '53.txt',\n",
       " '54.txt',\n",
       " '55.txt',\n",
       " '56.txt',\n",
       " '58.txt',\n",
       " '59.txt',\n",
       " '60.txt',\n",
       " '61.txt',\n",
       " '62.txt',\n",
       " '63.txt',\n",
       " '64.txt',\n",
       " '65.txt',\n",
       " '66.txt',\n",
       " '67.txt',\n",
       " '68.txt',\n",
       " '69.txt',\n",
       " '70.txt',\n",
       " '71.txt',\n",
       " '72.txt',\n",
       " '73.txt',\n",
       " '74.txt',\n",
       " '75.txt',\n",
       " '76.txt',\n",
       " '77.txt',\n",
       " '78.txt',\n",
       " '79.txt',\n",
       " '80.txt',\n",
       " '81.txt',\n",
       " '82.txt',\n",
       " '83.txt',\n",
       " '84.txt',\n",
       " '85.txt',\n",
       " '86.txt',\n",
       " '87.txt',\n",
       " '88.txt',\n",
       " '89.txt',\n",
       " '90.txt',\n",
       " '91.txt',\n",
       " '92.txt',\n",
       " '93.txt',\n",
       " '94.txt',\n",
       " '95.txt',\n",
       " '96.txt',\n",
       " '97.txt',\n",
       " '98.txt',\n",
       " '99.txt',\n",
       " 'blackcoffer1.xlsx']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load all file names\n",
    "at=[]\n",
    "pat = r\"C:\\Users\\drang\\crawl\"\n",
    "for (root,dirs,files) in os.walk(pat, topdown=True):\n",
    "    #print(files)\n",
    "    at.append(files)\n",
    "    pass\n",
    "at=at[:1][0]\n",
    "print(len(at))\n",
    "at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "552e472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add text to df\n",
    "pat = r\"C:\\Users\\drang\\crawl\"\n",
    "\n",
    "df.insert(column=\"text\", loc=2, value=0)\n",
    "\n",
    "for i in at[:-1]:\n",
    "    with open(pat +\"\\\\\"+ i, \"r\") as f:\n",
    "        #getting index of each urls and add text file to df\n",
    "        a=df[\"URL_ID\"].loc[lambda x: x==int(i.split('.')[0])].index\n",
    "        df[\"text\"].iloc[a]= f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c37390e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_text\n",
    "import re\n",
    "\n",
    "REPLACE_WITH_SPACE = re.compile(r\"\\n\")\n",
    "\n",
    "def preprocess_text(text, withspace=False, forcount=False):\n",
    "    if forcount:\n",
    "        REPLACE_NO_SPACE = re.compile(r\"[:!\\'?,|\\\"()\\[\\]]\")\n",
    "    else:\n",
    "        REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,|\\\"()\\[\\]]\")\n",
    "\n",
    "        \n",
    "    text = [REPLACE_WITH_SPACE.sub(\" \", line) for line in text]\n",
    "    text = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in text]\n",
    "\n",
    "    if withspace == True:\n",
    "        text= ' '.join(text)\n",
    "        #re.sub(r\"\\n\", \"\",reviews) \n",
    "        \n",
    "    else:\n",
    "        text= ''.join(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d6c12",
   "metadata": {},
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83246329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatized(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    worded=[]\n",
    "    \n",
    "    for base_word in data:\n",
    "        base_words = lemmatizer.lemmatize(base_word, pos='v')\n",
    "        \n",
    "        if base_words.endswith(\"ed\"):\n",
    "            base_words = base_words[:-2]\n",
    "        \n",
    "        elif base_words.endswith(\"es\"):\n",
    "            base_words = base_words[:-2]\n",
    "        worded.append(base_words)\n",
    "        \n",
    "    return worded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7387",
   "metadata": {},
   "source": [
    "## Treating Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b5d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_word_generator\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stop_word_gen():\n",
    "    stop_words=[]\n",
    "    stop_words.append(stopwords.words('english'))\n",
    "\n",
    "    pat = r'C:\\Users\\drang\\crawl\\New folder\\StopWords-20230217T091603Z-001\\StopWords'\n",
    "    for (root,dirs,files) in os.walk(pat, topdown=True):\n",
    "        pass\n",
    "        \n",
    "    for file in files:\n",
    "        with open(pat + '\\\\' + file, 'r', encoding='ISO-8859-1') as ot:\n",
    "\n",
    "            a = ot.read()\n",
    "            a= tokenizer(a)\n",
    "            stop_words.append(tokenizer(preprocess_text(a[1], withspace=True))[1])\n",
    "            \n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d68b4b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ernst',\n",
       "  'young',\n",
       "  'deloitte',\n",
       "  'touche',\n",
       "  'kpmg',\n",
       "  'pricewaterhousecoopers',\n",
       "  'pricewaterhouse',\n",
       "  'coopers'],\n",
       " ['afghani',\n",
       "  'afghanistan',\n",
       "  'ariary',\n",
       "  'madagascar',\n",
       "  'baht',\n",
       "  'thailand',\n",
       "  'balboa',\n",
       "  'panama',\n",
       "  'birr',\n",
       "  'ethiopia',\n",
       "  'bolivar',\n",
       "  'venezuela',\n",
       "  'boliviano',\n",
       "  'bolivia',\n",
       "  'cedi',\n",
       "  'ghana',\n",
       "  'colon',\n",
       "  'costa',\n",
       "  'rica',\n",
       "  'córdoba',\n",
       "  'nicaragua',\n",
       "  'dalasi',\n",
       "  'gambia',\n",
       "  'denar',\n",
       "  'macedonia',\n",
       "  'former',\n",
       "  'yug',\n",
       "  'rep',\n",
       "  'dinar',\n",
       "  'algeria',\n",
       "  'dirham',\n",
       "  'morocco',\n",
       "  'dobra',\n",
       "  'são',\n",
       "  'tom',\n",
       "  'and',\n",
       "  'príncipe',\n",
       "  'dong',\n",
       "  'vietnam',\n",
       "  'dram',\n",
       "  'armenia',\n",
       "  'escudo',\n",
       "  'cape',\n",
       "  'verde',\n",
       "  'euro',\n",
       "  'belgium',\n",
       "  'florin',\n",
       "  'aruba',\n",
       "  'forint',\n",
       "  'hungary',\n",
       "  'gourde',\n",
       "  'haiti',\n",
       "  'guarani',\n",
       "  'paraguay',\n",
       "  'gulden',\n",
       "  'netherlands',\n",
       "  'antilles',\n",
       "  'hryvnia',\n",
       "  'ukraine',\n",
       "  'kina',\n",
       "  'papua',\n",
       "  'new',\n",
       "  'guinea',\n",
       "  'kip',\n",
       "  'laos',\n",
       "  'konvertibilna',\n",
       "  'marka',\n",
       "  'bosnia-herzegovina',\n",
       "  'koruna',\n",
       "  'czech',\n",
       "  'republic',\n",
       "  'krona',\n",
       "  'sweden',\n",
       "  'krone',\n",
       "  'denmark',\n",
       "  'kroon',\n",
       "  'estonia',\n",
       "  'kuna',\n",
       "  'croatia',\n",
       "  'kwacha',\n",
       "  'zambia',\n",
       "  'kwanza',\n",
       "  'angola',\n",
       "  'kyat',\n",
       "  'myanmar',\n",
       "  'lari',\n",
       "  'georgia',\n",
       "  'lats',\n",
       "  'latvia',\n",
       "  'lek',\n",
       "  'albania',\n",
       "  'lempira',\n",
       "  'honduras',\n",
       "  'leone',\n",
       "  'sierra',\n",
       "  'leone',\n",
       "  'leu',\n",
       "  'romania',\n",
       "  'lev',\n",
       "  'bulgaria',\n",
       "  'lilangeni',\n",
       "  'swaziland',\n",
       "  'lira',\n",
       "  'lebanon',\n",
       "  'litas',\n",
       "  'lithuania',\n",
       "  'loti',\n",
       "  'lesotho',\n",
       "  'manat',\n",
       "  'azerbaijan',\n",
       "  'metical',\n",
       "  'mozambique',\n",
       "  'naira',\n",
       "  'nigeria',\n",
       "  'nakfa',\n",
       "  'eritrea',\n",
       "  'new',\n",
       "  'lira',\n",
       "  'turkey',\n",
       "  'new',\n",
       "  'sheqel',\n",
       "  'israel',\n",
       "  'ngultrum',\n",
       "  'bhutan',\n",
       "  'nuevo',\n",
       "  'sol',\n",
       "  'peru',\n",
       "  'ouguiya',\n",
       "  'mauritania',\n",
       "  'pataca',\n",
       "  'macau',\n",
       "  'peso',\n",
       "  'mexico',\n",
       "  'pound',\n",
       "  'egypt',\n",
       "  'pula',\n",
       "  'botswana',\n",
       "  'quetzal',\n",
       "  'guatemala',\n",
       "  'rand',\n",
       "  'south',\n",
       "  'africa',\n",
       "  'real',\n",
       "  'brazil',\n",
       "  'renminbi',\n",
       "  'china',\n",
       "  'rial',\n",
       "  'iran',\n",
       "  'riel',\n",
       "  'cambodia',\n",
       "  'ringgit',\n",
       "  'malaysia',\n",
       "  'riyal',\n",
       "  'saudi',\n",
       "  'arabia',\n",
       "  'ruble',\n",
       "  'russia',\n",
       "  'rufiyaa',\n",
       "  'maldives',\n",
       "  'rupee',\n",
       "  'india',\n",
       "  'rupee',\n",
       "  'pakistan',\n",
       "  'rupiah',\n",
       "  'indonesia',\n",
       "  'shilling',\n",
       "  'uganda',\n",
       "  'som',\n",
       "  'uzbekistan',\n",
       "  'somoni',\n",
       "  'tajikistan',\n",
       "  'special',\n",
       "  'drawing',\n",
       "  'rights',\n",
       "  'international',\n",
       "  'monetary',\n",
       "  'fund',\n",
       "  'taka',\n",
       "  'bangladesh',\n",
       "  'tala',\n",
       "  'western',\n",
       "  'samoa',\n",
       "  'tenge',\n",
       "  'kazakhstan',\n",
       "  'tugrik',\n",
       "  'mongolia',\n",
       "  'vatu',\n",
       "  'vanuatu',\n",
       "  'won',\n",
       "  'korea',\n",
       "  'south',\n",
       "  'yen',\n",
       "  'japan',\n",
       "  'zloty',\n",
       "  'poland'],\n",
       " ['hundred',\n",
       "  'denominations',\n",
       "  'thousand',\n",
       "  'million',\n",
       "  'billion',\n",
       "  'trillion',\n",
       "  'date',\n",
       "  'time',\n",
       "  'related',\n",
       "  'annual',\n",
       "  'annually',\n",
       "  'annum',\n",
       "  'year',\n",
       "  'yearly',\n",
       "  'quarter',\n",
       "  'quarterly',\n",
       "  'qtr',\n",
       "  'month',\n",
       "  'monthly',\n",
       "  'week',\n",
       "  'weekly',\n",
       "  'day',\n",
       "  'daily',\n",
       "  'january',\n",
       "  'calendar',\n",
       "  'february',\n",
       "  'march',\n",
       "  'april',\n",
       "  'may',\n",
       "  'june',\n",
       "  'july',\n",
       "  'august',\n",
       "  'september',\n",
       "  'october',\n",
       "  'november',\n",
       "  'december',\n",
       "  'jan',\n",
       "  'feb',\n",
       "  'mar',\n",
       "  'apr',\n",
       "  'may',\n",
       "  'jun',\n",
       "  'jul',\n",
       "  'aug',\n",
       "  'sep',\n",
       "  'sept',\n",
       "  'oct',\n",
       "  'nov',\n",
       "  'dec',\n",
       "  'monday',\n",
       "  'tuesday',\n",
       "  'wednesday',\n",
       "  'thursday',\n",
       "  'friday',\n",
       "  'saturday',\n",
       "  'sunday',\n",
       "  'one',\n",
       "  'numbers',\n",
       "  'two',\n",
       "  'three',\n",
       "  'four',\n",
       "  'five',\n",
       "  'six',\n",
       "  'seven',\n",
       "  'eight',\n",
       "  'nine',\n",
       "  'ten',\n",
       "  'eleven',\n",
       "  'twelve',\n",
       "  'thirteen',\n",
       "  'fourteen',\n",
       "  'fifteen',\n",
       "  'sixteen',\n",
       "  'seventeen',\n",
       "  'eighteen',\n",
       "  'nineteen',\n",
       "  'twenty',\n",
       "  'thirty',\n",
       "  'forty',\n",
       "  'fifty',\n",
       "  'sixty',\n",
       "  'seventy',\n",
       "  'eighty',\n",
       "  'ninety',\n",
       "  'first',\n",
       "  'second',\n",
       "  'third',\n",
       "  'fourth',\n",
       "  'fifth',\n",
       "  'sixth',\n",
       "  'seventh',\n",
       "  'eighth',\n",
       "  'ninth',\n",
       "  'tenth',\n",
       "  'i',\n",
       "  'roman',\n",
       "  'numerals',\n",
       "  'ii',\n",
       "  'iii',\n",
       "  'iv',\n",
       "  'v',\n",
       "  'vi',\n",
       "  'vii',\n",
       "  'viii',\n",
       "  'ix',\n",
       "  'x',\n",
       "  'xi',\n",
       "  'xii',\n",
       "  'xiii',\n",
       "  'xiv',\n",
       "  'xv',\n",
       "  'xvi',\n",
       "  'xvii',\n",
       "  'xviii',\n",
       "  'xix',\n",
       "  'xx'],\n",
       " ['about',\n",
       "  'above',\n",
       "  'after',\n",
       "  'again',\n",
       "  'all',\n",
       "  'am',\n",
       "  'among',\n",
       "  'an',\n",
       "  'and',\n",
       "  'any',\n",
       "  'are',\n",
       "  'as',\n",
       "  'at',\n",
       "  'be',\n",
       "  'because',\n",
       "  'been',\n",
       "  'before',\n",
       "  'being',\n",
       "  'below',\n",
       "  'between',\n",
       "  'both',\n",
       "  'but',\n",
       "  'by',\n",
       "  'can',\n",
       "  'did',\n",
       "  'do',\n",
       "  'does',\n",
       "  'doing',\n",
       "  'down',\n",
       "  'during',\n",
       "  'each',\n",
       "  'few',\n",
       "  'for',\n",
       "  'from',\n",
       "  'further',\n",
       "  'had',\n",
       "  'has',\n",
       "  'have',\n",
       "  'having',\n",
       "  'he',\n",
       "  'her',\n",
       "  'here',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'him',\n",
       "  'himself',\n",
       "  'his',\n",
       "  'how',\n",
       "  'if',\n",
       "  'in',\n",
       "  'into',\n",
       "  'is',\n",
       "  'it',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'just',\n",
       "  'me',\n",
       "  'more',\n",
       "  'most',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'now',\n",
       "  'of',\n",
       "  'off',\n",
       "  'on',\n",
       "  'once',\n",
       "  'only',\n",
       "  'or',\n",
       "  'other',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'out',\n",
       "  'over',\n",
       "  'own',\n",
       "  'same',\n",
       "  'she',\n",
       "  'should',\n",
       "  'so',\n",
       "  'some',\n",
       "  'such',\n",
       "  'than',\n",
       "  'that',\n",
       "  'the',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'them',\n",
       "  'themselves',\n",
       "  'then',\n",
       "  'there',\n",
       "  'these',\n",
       "  'they',\n",
       "  'this',\n",
       "  'those',\n",
       "  'through',\n",
       "  'to',\n",
       "  'too',\n",
       "  'under',\n",
       "  'until',\n",
       "  'up',\n",
       "  'very',\n",
       "  'was',\n",
       "  'we',\n",
       "  'were',\n",
       "  'what',\n",
       "  'when',\n",
       "  'where',\n",
       "  'which',\n",
       "  'while',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'why',\n",
       "  'with',\n",
       "  'you',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves'],\n",
       " ['a',\n",
       "  'as',\n",
       "  'able',\n",
       "  'about',\n",
       "  'above',\n",
       "  'according',\n",
       "  'accordingly',\n",
       "  'across',\n",
       "  'actually',\n",
       "  'after',\n",
       "  'afterwards',\n",
       "  'again',\n",
       "  'against',\n",
       "  'aint',\n",
       "  'all',\n",
       "  'allow',\n",
       "  'allows',\n",
       "  'almost',\n",
       "  'alone',\n",
       "  'along',\n",
       "  'already',\n",
       "  'also',\n",
       "  'although',\n",
       "  'always',\n",
       "  'am',\n",
       "  'among',\n",
       "  'amongst',\n",
       "  'an',\n",
       "  'and',\n",
       "  'another',\n",
       "  'any',\n",
       "  'anybody',\n",
       "  'anyhow',\n",
       "  'anyone',\n",
       "  'anything',\n",
       "  'anyway',\n",
       "  'anyways',\n",
       "  'anywhere',\n",
       "  'apart',\n",
       "  'appear',\n",
       "  'appreciate',\n",
       "  'appropriate',\n",
       "  'are',\n",
       "  'arent',\n",
       "  'around',\n",
       "  'as',\n",
       "  'aside',\n",
       "  'ask',\n",
       "  'asking',\n",
       "  'associated',\n",
       "  'at',\n",
       "  'available',\n",
       "  'away',\n",
       "  'awfully',\n",
       "  'b',\n",
       "  'be',\n",
       "  'became',\n",
       "  'because',\n",
       "  'become',\n",
       "  'becomes',\n",
       "  'becoming',\n",
       "  'been',\n",
       "  'before',\n",
       "  'beforehand',\n",
       "  'behind',\n",
       "  'being',\n",
       "  'believe',\n",
       "  'below',\n",
       "  'beside',\n",
       "  'besides',\n",
       "  'best',\n",
       "  'better',\n",
       "  'between',\n",
       "  'beyond',\n",
       "  'both',\n",
       "  'brief',\n",
       "  'but',\n",
       "  'by',\n",
       "  'c',\n",
       "  'cmon',\n",
       "  'cs',\n",
       "  'came',\n",
       "  'can',\n",
       "  'cant',\n",
       "  'can',\n",
       "  'not',\n",
       "  'cant',\n",
       "  'cause',\n",
       "  'causes',\n",
       "  'certain',\n",
       "  'certainly',\n",
       "  'changes',\n",
       "  'clearly',\n",
       "  'co',\n",
       "  'com',\n",
       "  'come',\n",
       "  'comes',\n",
       "  'concerning',\n",
       "  'consequently',\n",
       "  'consider',\n",
       "  'considering',\n",
       "  'contain',\n",
       "  'containing',\n",
       "  'contains',\n",
       "  'corresponding',\n",
       "  'could',\n",
       "  'couldnt',\n",
       "  'course',\n",
       "  'currently',\n",
       "  'd',\n",
       "  'definitely',\n",
       "  'described',\n",
       "  'despite',\n",
       "  'did',\n",
       "  'didnt',\n",
       "  'different',\n",
       "  'do',\n",
       "  'does',\n",
       "  'doesnt',\n",
       "  'doing',\n",
       "  'dont',\n",
       "  'done',\n",
       "  'down',\n",
       "  'downwards',\n",
       "  'during',\n",
       "  'e',\n",
       "  'each',\n",
       "  'edu',\n",
       "  'eg',\n",
       "  'eight',\n",
       "  'either',\n",
       "  'else',\n",
       "  'elsewhere',\n",
       "  'enough',\n",
       "  'entirely',\n",
       "  'especially',\n",
       "  'et',\n",
       "  'etc',\n",
       "  'even',\n",
       "  'ever',\n",
       "  'every',\n",
       "  'everybody',\n",
       "  'everyone',\n",
       "  'everything',\n",
       "  'everywhere',\n",
       "  'ex',\n",
       "  'exactly',\n",
       "  'example',\n",
       "  'except',\n",
       "  'f',\n",
       "  'far',\n",
       "  'few',\n",
       "  'fifth',\n",
       "  'first',\n",
       "  'five',\n",
       "  'followed',\n",
       "  'following',\n",
       "  'follows',\n",
       "  'for',\n",
       "  'former',\n",
       "  'formerly',\n",
       "  'forth',\n",
       "  'four',\n",
       "  'from',\n",
       "  'further',\n",
       "  'furthermore',\n",
       "  'g',\n",
       "  'get',\n",
       "  'gets',\n",
       "  'getting',\n",
       "  'given',\n",
       "  'gives',\n",
       "  'go',\n",
       "  'goes',\n",
       "  'going',\n",
       "  'gone',\n",
       "  'got',\n",
       "  'gotten',\n",
       "  'greetings',\n",
       "  'h',\n",
       "  'had',\n",
       "  'hadnt',\n",
       "  'happens',\n",
       "  'hardly',\n",
       "  'has',\n",
       "  'hasnt',\n",
       "  'have',\n",
       "  'havent',\n",
       "  'having',\n",
       "  'he',\n",
       "  'hes',\n",
       "  'hello',\n",
       "  'help',\n",
       "  'hence',\n",
       "  'her',\n",
       "  'here',\n",
       "  'heres',\n",
       "  'hereafter',\n",
       "  'hereby',\n",
       "  'herein',\n",
       "  'hereupon',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'hi',\n",
       "  'him',\n",
       "  'himself',\n",
       "  'his',\n",
       "  'hither',\n",
       "  'hopefully',\n",
       "  'how',\n",
       "  'howbeit',\n",
       "  'however',\n",
       "  'i',\n",
       "  'id',\n",
       "  'ill',\n",
       "  'im',\n",
       "  'ive',\n",
       "  'ie',\n",
       "  'if',\n",
       "  'ignored',\n",
       "  'immediate',\n",
       "  'in',\n",
       "  'inasmuch',\n",
       "  'inc',\n",
       "  'indeed',\n",
       "  'indicate',\n",
       "  'indicated',\n",
       "  'indicates',\n",
       "  'inner',\n",
       "  'insofar',\n",
       "  'instead',\n",
       "  'into',\n",
       "  'inward',\n",
       "  'is',\n",
       "  'isnt',\n",
       "  'it',\n",
       "  'itd',\n",
       "  'itll',\n",
       "  'its',\n",
       "  'its',\n",
       "  'itself',\n",
       "  'j',\n",
       "  'just',\n",
       "  'k',\n",
       "  'keep',\n",
       "  'keeps',\n",
       "  'kept',\n",
       "  'know',\n",
       "  'knows',\n",
       "  'known',\n",
       "  'l',\n",
       "  'last',\n",
       "  'lately',\n",
       "  'later',\n",
       "  'latter',\n",
       "  'latterly',\n",
       "  'least',\n",
       "  'less',\n",
       "  'lest',\n",
       "  'let',\n",
       "  'lets',\n",
       "  'like',\n",
       "  'liked',\n",
       "  'likely',\n",
       "  'little',\n",
       "  'look',\n",
       "  'looking',\n",
       "  'looks',\n",
       "  'ltd',\n",
       "  'm',\n",
       "  'mainly',\n",
       "  'many',\n",
       "  'may',\n",
       "  'maybe',\n",
       "  'me',\n",
       "  'mean',\n",
       "  'meanwhile',\n",
       "  'merely',\n",
       "  'might',\n",
       "  'more',\n",
       "  'moreover',\n",
       "  'most',\n",
       "  'mostly',\n",
       "  'much',\n",
       "  'must',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'n',\n",
       "  'name',\n",
       "  'namely',\n",
       "  'nd',\n",
       "  'near',\n",
       "  'nearly',\n",
       "  'necessary',\n",
       "  'need',\n",
       "  'needs',\n",
       "  'neither',\n",
       "  'never',\n",
       "  'nevertheless',\n",
       "  'new',\n",
       "  'next',\n",
       "  'nine',\n",
       "  'no',\n",
       "  'nobody',\n",
       "  'non',\n",
       "  'none',\n",
       "  'noone',\n",
       "  'nor',\n",
       "  'normally',\n",
       "  'not',\n",
       "  'nothing',\n",
       "  'novel',\n",
       "  'now',\n",
       "  'nowhere',\n",
       "  'o',\n",
       "  'obviously',\n",
       "  'of',\n",
       "  'off',\n",
       "  'often',\n",
       "  'oh',\n",
       "  'ok',\n",
       "  'okay',\n",
       "  'old',\n",
       "  'on',\n",
       "  'once',\n",
       "  'one',\n",
       "  'ones',\n",
       "  'only',\n",
       "  'onto',\n",
       "  'or',\n",
       "  'other',\n",
       "  'others',\n",
       "  'otherwise',\n",
       "  'ought',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'out',\n",
       "  'outside',\n",
       "  'over',\n",
       "  'overall',\n",
       "  'own',\n",
       "  'p',\n",
       "  'particular',\n",
       "  'particularly',\n",
       "  'per',\n",
       "  'perhaps',\n",
       "  'placed',\n",
       "  'please',\n",
       "  'plus',\n",
       "  'possible',\n",
       "  'presumably',\n",
       "  'probably',\n",
       "  'provides',\n",
       "  'q',\n",
       "  'que',\n",
       "  'quite',\n",
       "  'qv',\n",
       "  'r',\n",
       "  'rather',\n",
       "  'rd',\n",
       "  're',\n",
       "  'really',\n",
       "  'reasonably',\n",
       "  'regarding',\n",
       "  'regardless',\n",
       "  'regards',\n",
       "  'relatively',\n",
       "  'respectively',\n",
       "  'right',\n",
       "  's',\n",
       "  'said',\n",
       "  'same',\n",
       "  'saw',\n",
       "  'say',\n",
       "  'saying',\n",
       "  'says',\n",
       "  'second',\n",
       "  'secondly',\n",
       "  'see',\n",
       "  'seeing',\n",
       "  'seem',\n",
       "  'seemed',\n",
       "  'seeming',\n",
       "  'seems',\n",
       "  'seen',\n",
       "  'self',\n",
       "  'selves',\n",
       "  'sensible',\n",
       "  'sent',\n",
       "  'serious',\n",
       "  'seriously',\n",
       "  'seven',\n",
       "  'several',\n",
       "  'shall',\n",
       "  'she',\n",
       "  'should',\n",
       "  'shouldnt',\n",
       "  'since',\n",
       "  'six',\n",
       "  'so',\n",
       "  'some',\n",
       "  'somebody',\n",
       "  'somehow',\n",
       "  'someone',\n",
       "  'something',\n",
       "  'sometime',\n",
       "  'sometimes',\n",
       "  'somewhat',\n",
       "  'somewhere',\n",
       "  'soon',\n",
       "  'sorry',\n",
       "  'specified',\n",
       "  'specify',\n",
       "  'specifying',\n",
       "  'still',\n",
       "  'sub',\n",
       "  'such',\n",
       "  'sup',\n",
       "  'sure',\n",
       "  't',\n",
       "  'ts',\n",
       "  'take',\n",
       "  'taken',\n",
       "  'tell',\n",
       "  'tends',\n",
       "  'th',\n",
       "  'than',\n",
       "  'thank',\n",
       "  'thanks',\n",
       "  'thanx',\n",
       "  'that',\n",
       "  'thats',\n",
       "  'thats',\n",
       "  'the',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'them',\n",
       "  'themselves',\n",
       "  'then',\n",
       "  'thence',\n",
       "  'there',\n",
       "  'theres',\n",
       "  'thereafter',\n",
       "  'thereby',\n",
       "  'therefore',\n",
       "  'therein',\n",
       "  'theres',\n",
       "  'thereupon',\n",
       "  'these',\n",
       "  'they',\n",
       "  'theyd',\n",
       "  'theyll',\n",
       "  'theyre',\n",
       "  'theyve',\n",
       "  'think',\n",
       "  'third',\n",
       "  'this',\n",
       "  'thorough',\n",
       "  'thoroughly',\n",
       "  'those',\n",
       "  'though',\n",
       "  'three',\n",
       "  'through',\n",
       "  'throughout',\n",
       "  'thru',\n",
       "  'thus',\n",
       "  'to',\n",
       "  'together',\n",
       "  'too',\n",
       "  'took',\n",
       "  'toward',\n",
       "  'towards',\n",
       "  'tried',\n",
       "  'tries',\n",
       "  'truly',\n",
       "  'try',\n",
       "  'trying',\n",
       "  'twice',\n",
       "  'two',\n",
       "  'u',\n",
       "  'un',\n",
       "  'under',\n",
       "  'unfortunately',\n",
       "  'unless',\n",
       "  'unlikely',\n",
       "  'until',\n",
       "  'unto',\n",
       "  'up',\n",
       "  'upon',\n",
       "  'us',\n",
       "  'use',\n",
       "  'used',\n",
       "  'useful',\n",
       "  'uses',\n",
       "  'using',\n",
       "  'usually',\n",
       "  'uucp',\n",
       "  'v',\n",
       "  'value',\n",
       "  'various',\n",
       "  'very',\n",
       "  'via',\n",
       "  'viz',\n",
       "  'vs',\n",
       "  'w',\n",
       "  'want',\n",
       "  'wants',\n",
       "  'was',\n",
       "  'wasnt',\n",
       "  'way',\n",
       "  'we',\n",
       "  'wed',\n",
       "  'well',\n",
       "  'were',\n",
       "  'weve',\n",
       "  'welcome',\n",
       "  'well',\n",
       "  'went',\n",
       "  'were',\n",
       "  'werent',\n",
       "  'what',\n",
       "  'whats',\n",
       "  'whatever',\n",
       "  'when',\n",
       "  'whence',\n",
       "  'whenever',\n",
       "  'where',\n",
       "  'wheres',\n",
       "  'whereafter',\n",
       "  'whereas',\n",
       "  'whereby',\n",
       "  'wherein',\n",
       "  'whereupon',\n",
       "  'wherever',\n",
       "  'whether',\n",
       "  'which',\n",
       "  'while',\n",
       "  'whither',\n",
       "  'who',\n",
       "  'whos',\n",
       "  'whoever',\n",
       "  'whole',\n",
       "  'whom',\n",
       "  'whose',\n",
       "  'why',\n",
       "  'will',\n",
       "  'willing',\n",
       "  'wish',\n",
       "  'with',\n",
       "  'within',\n",
       "  'without',\n",
       "  'wont',\n",
       "  'wonder',\n",
       "  'would',\n",
       "  'would',\n",
       "  'wouldnt',\n",
       "  'x',\n",
       "  'y',\n",
       "  'yes',\n",
       "  'yet',\n",
       "  'you',\n",
       "  'youd',\n",
       "  'youll',\n",
       "  'youre',\n",
       "  'youve',\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'z',\n",
       "  'zero'],\n",
       " ['united',\n",
       "  'geographic',\n",
       "  'state',\n",
       "  'north',\n",
       "  'south',\n",
       "  'east',\n",
       "  'northeast',\n",
       "  'northwest',\n",
       "  'southeast',\n",
       "  'southwest',\n",
       "  'west',\n",
       "  'ocean',\n",
       "  'sea',\n",
       "  'lake',\n",
       "  'river',\n",
       "  'creek',\n",
       "  'gulf',\n",
       "  'mountain',\n",
       "  'street',\n",
       "  'boulevard',\n",
       "  'blvd',\n",
       "  'parkway',\n",
       "  'city',\n",
       "  'county',\n",
       "  'country',\n",
       "  'pacific',\n",
       "  'atlantic',\n",
       "  'indian',\n",
       "  'mediterranean',\n",
       "  'commonwealth',\n",
       "  'america',\n",
       "  'american',\n",
       "  'york',\n",
       "  'cities',\n",
       "  'chicago',\n",
       "  'las',\n",
       "  'vegas',\n",
       "  'los',\n",
       "  'angeles',\n",
       "  'milwaukee',\n",
       "  'sunnyvale',\n",
       "  'fremont',\n",
       "  'cincinnati',\n",
       "  'philadelphia',\n",
       "  'miami',\n",
       "  'dallas',\n",
       "  'fort',\n",
       "  'boston',\n",
       "  'houston',\n",
       "  'washington',\n",
       "  'atlanta',\n",
       "  'detroit',\n",
       "  'san',\n",
       "  'fransico',\n",
       "  'phoenix',\n",
       "  'seattle',\n",
       "  'diego',\n",
       "  'minneapolis',\n",
       "  'memphis',\n",
       "  'denver',\n",
       "  'st',\n",
       "  'louis',\n",
       "  'pittsburgh',\n",
       "  'manhattan',\n",
       "  'hollywood',\n",
       "  'columbus',\n",
       "  'indianapolis',\n",
       "  'mumbai',\n",
       "  'karachi',\n",
       "  'ontario',\n",
       "  'toronto',\n",
       "  'cambridge',\n",
       "  'delhi',\n",
       "  'sao',\n",
       "  'paulo',\n",
       "  'shanghai',\n",
       "  'moscow',\n",
       "  'seoul',\n",
       "  'istanbul',\n",
       "  'tokyo',\n",
       "  'jakarta',\n",
       "  'beijing',\n",
       "  'london',\n",
       "  'luxembourg',\n",
       "  'singapore',\n",
       "  'republic',\n",
       "  'countries',\n",
       "  'china',\n",
       "  'india',\n",
       "  'indonesia',\n",
       "  'brazil',\n",
       "  'brazilian',\n",
       "  'pakistan',\n",
       "  'bangladesh',\n",
       "  'russia',\n",
       "  'nigeria',\n",
       "  'nova',\n",
       "  'scotia',\n",
       "  'japan',\n",
       "  'malaysia',\n",
       "  'mexico',\n",
       "  'mexican',\n",
       "  'philippines',\n",
       "  'vietnam',\n",
       "  'germany',\n",
       "  'france',\n",
       "  'korea',\n",
       "  'spain',\n",
       "  'argentina',\n",
       "  'bermuda',\n",
       "  'columbia',\n",
       "  'puerto',\n",
       "  'canada',\n",
       "  'canadian',\n",
       "  'africa',\n",
       "  'african',\n",
       "  'chile',\n",
       "  'chilean',\n",
       "  'europe',\n",
       "  'european',\n",
       "  'usa',\n",
       "  'italy',\n",
       "  'italian',\n",
       "  'australia',\n",
       "  'netherlands',\n",
       "  'norway',\n",
       "  'portugal',\n",
       "  'taiwan',\n",
       "  'argentina',\n",
       "  'switzerland',\n",
       "  'swiss',\n",
       "  'suisse',\n",
       "  'denmark',\n",
       "  'czeck',\n",
       "  'hungary',\n",
       "  'finland',\n",
       "  'scotland',\n",
       "  'costa',\n",
       "  'rica',\n",
       "  'sweden',\n",
       "  'belgium',\n",
       "  'austria',\n",
       "  'turkey',\n",
       "  'poland',\n",
       "  'indonesia',\n",
       "  'thailand',\n",
       "  'venezuela',\n",
       "  'asia',\n",
       "  'zealand',\n",
       "  'japanese',\n",
       "  'latin',\n",
       "  'british',\n",
       "  'chinese',\n",
       "  'alabama',\n",
       "  'states',\n",
       "  'alaska',\n",
       "  'arizona',\n",
       "  'arkansas',\n",
       "  'california',\n",
       "  'colorado',\n",
       "  'connecticut',\n",
       "  'delaware',\n",
       "  'florida',\n",
       "  'georgia',\n",
       "  'hawaii',\n",
       "  'idaho',\n",
       "  'illinois',\n",
       "  'indiana',\n",
       "  'iowa',\n",
       "  'kansas',\n",
       "  'kentucky',\n",
       "  'louisiana',\n",
       "  'maine',\n",
       "  'maryland',\n",
       "  'massachusetts',\n",
       "  'michigan',\n",
       "  'minnesota',\n",
       "  'mississippi',\n",
       "  'missouri',\n",
       "  'montana',\n",
       "  'nebraska',\n",
       "  'nevada',\n",
       "  'hampshire',\n",
       "  'jersey',\n",
       "  'mexico',\n",
       "  'new',\n",
       "  'york',\n",
       "  'ohio',\n",
       "  'oklahoma',\n",
       "  'oregon',\n",
       "  'pennsylvania',\n",
       "  'rhode',\n",
       "  'island',\n",
       "  'carolina',\n",
       "  'dakota',\n",
       "  'tennessee',\n",
       "  'texas',\n",
       "  'utah',\n",
       "  'vermont',\n",
       "  'virginia',\n",
       "  'washington',\n",
       "  'wisconsin',\n",
       "  'wyoming'],\n",
       " ['smith',\n",
       "  'surnames',\n",
       "  'from',\n",
       "  '1990',\n",
       "  'census',\n",
       "  '>',\n",
       "  '002',\n",
       "  '%',\n",
       "  'wwwcensusgovgenealogy/names/distalllast',\n",
       "  'johnson',\n",
       "  'williams',\n",
       "  'jones',\n",
       "  'brown',\n",
       "  'davis',\n",
       "  'miller',\n",
       "  'wilson',\n",
       "  'moore',\n",
       "  'taylor',\n",
       "  'anderson',\n",
       "  'thomas',\n",
       "  'jackson',\n",
       "  'white',\n",
       "  'harris',\n",
       "  'martin',\n",
       "  'thompson',\n",
       "  'garcia',\n",
       "  'martinez',\n",
       "  'robinson',\n",
       "  'clark',\n",
       "  'rodriguez',\n",
       "  'lewis',\n",
       "  'lee',\n",
       "  'walker',\n",
       "  'hall',\n",
       "  'allen',\n",
       "  'young',\n",
       "  'hernandez',\n",
       "  'king',\n",
       "  'wright',\n",
       "  'lopez',\n",
       "  'hill',\n",
       "  'scott',\n",
       "  'green',\n",
       "  'adams',\n",
       "  'baker',\n",
       "  'gonzalez',\n",
       "  'nelson',\n",
       "  'carter',\n",
       "  'mitchell',\n",
       "  'perez',\n",
       "  'roberts',\n",
       "  'turner',\n",
       "  'phillips',\n",
       "  'campbell',\n",
       "  'parker',\n",
       "  'evans',\n",
       "  'edwards',\n",
       "  'collins',\n",
       "  'stewart',\n",
       "  'sanchez',\n",
       "  'morris',\n",
       "  'rogers',\n",
       "  'reed',\n",
       "  'cook',\n",
       "  'morgan',\n",
       "  'bell',\n",
       "  'murphy',\n",
       "  'bailey',\n",
       "  'rivera',\n",
       "  'cooper',\n",
       "  'richardson',\n",
       "  'cox',\n",
       "  'howard',\n",
       "  'ward',\n",
       "  'torres',\n",
       "  'peterson',\n",
       "  'gray',\n",
       "  'ramirez',\n",
       "  'james',\n",
       "  'watson',\n",
       "  'brooks',\n",
       "  'kelly',\n",
       "  'sanders',\n",
       "  'price',\n",
       "  'bennett',\n",
       "  'wood',\n",
       "  'barnes',\n",
       "  'ross',\n",
       "  'henderson',\n",
       "  'coleman',\n",
       "  'jenkins',\n",
       "  'perry',\n",
       "  'powell',\n",
       "  'long',\n",
       "  'patterson',\n",
       "  'hughes',\n",
       "  'flores',\n",
       "  'washington',\n",
       "  'butler',\n",
       "  'simmons',\n",
       "  'foster',\n",
       "  'gonzales',\n",
       "  'bryant',\n",
       "  'alexander',\n",
       "  'russell',\n",
       "  'griffin',\n",
       "  'diaz',\n",
       "  'hayes',\n",
       "  'myers',\n",
       "  'ford',\n",
       "  'hamilton',\n",
       "  'graham',\n",
       "  'sullivan',\n",
       "  'wallace',\n",
       "  'woods',\n",
       "  'cole',\n",
       "  'west',\n",
       "  'jordan',\n",
       "  'owens',\n",
       "  'reynolds',\n",
       "  'fisher',\n",
       "  'ellis',\n",
       "  'harrison',\n",
       "  'gibson',\n",
       "  'mcdonald',\n",
       "  'cruz',\n",
       "  'marshall',\n",
       "  'ortiz',\n",
       "  'gomez',\n",
       "  'murray',\n",
       "  'freeman',\n",
       "  'wells',\n",
       "  'webb',\n",
       "  'simpson',\n",
       "  'stevens',\n",
       "  'tucker',\n",
       "  'porter',\n",
       "  'hunter',\n",
       "  'hicks',\n",
       "  'crawford',\n",
       "  'henry',\n",
       "  'boyd',\n",
       "  'mason',\n",
       "  'morales',\n",
       "  'kennedy',\n",
       "  'warren',\n",
       "  'dixon',\n",
       "  'ramos',\n",
       "  'reyes',\n",
       "  'burns',\n",
       "  'gordon',\n",
       "  'shaw',\n",
       "  'holmes',\n",
       "  'rice',\n",
       "  'robertson',\n",
       "  'hunt',\n",
       "  'black',\n",
       "  'daniels',\n",
       "  'palmer',\n",
       "  'mills',\n",
       "  'nichols',\n",
       "  'grant',\n",
       "  'knight',\n",
       "  'ferguson',\n",
       "  'rose',\n",
       "  'stone',\n",
       "  'hawkins',\n",
       "  'dunn',\n",
       "  'perkins',\n",
       "  'hudson',\n",
       "  'spencer',\n",
       "  'gardner',\n",
       "  'stephens',\n",
       "  'payne',\n",
       "  'pierce',\n",
       "  'berry',\n",
       "  'matthews',\n",
       "  'arnold',\n",
       "  'wagner',\n",
       "  'willis',\n",
       "  'ray',\n",
       "  'watkins',\n",
       "  'olson',\n",
       "  'carroll',\n",
       "  'duncan',\n",
       "  'snyder',\n",
       "  'hart',\n",
       "  'cunningham',\n",
       "  'bradley',\n",
       "  'lane',\n",
       "  'andrews',\n",
       "  'ruiz',\n",
       "  'harper',\n",
       "  'fox',\n",
       "  'riley',\n",
       "  'armstrong',\n",
       "  'carpenter',\n",
       "  'weaver',\n",
       "  'greene',\n",
       "  'lawrence',\n",
       "  'elliott',\n",
       "  'chavez',\n",
       "  'sims',\n",
       "  'austin',\n",
       "  'peters',\n",
       "  'kelley',\n",
       "  'franklin',\n",
       "  'lawson',\n",
       "  'fields',\n",
       "  'gutierrez',\n",
       "  'ryan',\n",
       "  'schmidt',\n",
       "  'carr',\n",
       "  'vasquez',\n",
       "  'castillo',\n",
       "  'wheeler',\n",
       "  'chapman',\n",
       "  'oliver',\n",
       "  'montgomery',\n",
       "  'richards',\n",
       "  'williamson',\n",
       "  'johnston',\n",
       "  'banks',\n",
       "  'meyer',\n",
       "  'bishop',\n",
       "  'mccoy',\n",
       "  'howell',\n",
       "  'alvarez',\n",
       "  'morrison',\n",
       "  'hansen',\n",
       "  'fernandez',\n",
       "  'garza',\n",
       "  'harvey',\n",
       "  'little',\n",
       "  'burton',\n",
       "  'stanley',\n",
       "  'nguyen',\n",
       "  'george',\n",
       "  'jacobs',\n",
       "  'reid',\n",
       "  'kim',\n",
       "  'fuller',\n",
       "  'lynch',\n",
       "  'dean',\n",
       "  'gilbert',\n",
       "  'garrett',\n",
       "  'romero',\n",
       "  'welch',\n",
       "  'larson',\n",
       "  'frazier',\n",
       "  'burke',\n",
       "  'hanson',\n",
       "  'day',\n",
       "  'mendoza',\n",
       "  'moreno',\n",
       "  'bowman',\n",
       "  'medina',\n",
       "  'fowler',\n",
       "  'brewer',\n",
       "  'hoffman',\n",
       "  'carlson',\n",
       "  'silva',\n",
       "  'pearson',\n",
       "  'holland',\n",
       "  'douglas',\n",
       "  'fleming',\n",
       "  'jensen',\n",
       "  'vargas',\n",
       "  'byrd',\n",
       "  'davidson',\n",
       "  'hopkins',\n",
       "  'may',\n",
       "  'terry',\n",
       "  'herrera',\n",
       "  'wade',\n",
       "  'soto',\n",
       "  'walters',\n",
       "  'curtis',\n",
       "  'neal',\n",
       "  'caldwell',\n",
       "  'lowe',\n",
       "  'jennings',\n",
       "  'barnett',\n",
       "  'graves',\n",
       "  'jimenez',\n",
       "  'horton',\n",
       "  'shelton',\n",
       "  'barrett',\n",
       "  'obrien',\n",
       "  'castro',\n",
       "  'sutton',\n",
       "  'gregory',\n",
       "  'mckinney',\n",
       "  'lucas',\n",
       "  'miles',\n",
       "  'craig',\n",
       "  'rodriquez',\n",
       "  'chambers',\n",
       "  'holt',\n",
       "  'lambert',\n",
       "  'fletcher',\n",
       "  'watts',\n",
       "  'bates',\n",
       "  'hale',\n",
       "  'rhodes',\n",
       "  'pena',\n",
       "  'beck',\n",
       "  'newman',\n",
       "  'haynes',\n",
       "  'mcdaniel',\n",
       "  'mendez',\n",
       "  'bush',\n",
       "  'vaughn',\n",
       "  'parks',\n",
       "  'dawson',\n",
       "  'santiago',\n",
       "  'norris',\n",
       "  'hardy',\n",
       "  'love',\n",
       "  'steele',\n",
       "  'curry',\n",
       "  'powers',\n",
       "  'schultz',\n",
       "  'barker',\n",
       "  'guzman',\n",
       "  'page',\n",
       "  'munoz',\n",
       "  'ball',\n",
       "  'keller',\n",
       "  'chandler',\n",
       "  'weber',\n",
       "  'leonard',\n",
       "  'walsh',\n",
       "  'lyons',\n",
       "  'ramsey',\n",
       "  'wolfe',\n",
       "  'schneider',\n",
       "  'mullins',\n",
       "  'benson',\n",
       "  'sharp',\n",
       "  'bowen',\n",
       "  'daniel',\n",
       "  'barber',\n",
       "  'cummings',\n",
       "  'hines',\n",
       "  'baldwin',\n",
       "  'griffith',\n",
       "  'valdez',\n",
       "  'hubbard',\n",
       "  'salazar',\n",
       "  'reeves',\n",
       "  'warner',\n",
       "  'stevenson',\n",
       "  'burgess',\n",
       "  'santos',\n",
       "  'tate',\n",
       "  'cross',\n",
       "  'garner',\n",
       "  'mann',\n",
       "  'mack',\n",
       "  'moss',\n",
       "  'thornton',\n",
       "  'dennis',\n",
       "  'mcgee',\n",
       "  'farmer',\n",
       "  'delgado',\n",
       "  'aguilar',\n",
       "  'vega',\n",
       "  'glover',\n",
       "  'manning',\n",
       "  'cohen',\n",
       "  'harmon',\n",
       "  'rodgers',\n",
       "  'robbins',\n",
       "  'newton',\n",
       "  'todd',\n",
       "  'blair',\n",
       "  'higgins',\n",
       "  'ingram',\n",
       "  'reese',\n",
       "  'cannon',\n",
       "  'strickland',\n",
       "  'townsend',\n",
       "  'potter',\n",
       "  'goodwin',\n",
       "  'walton',\n",
       "  'rowe',\n",
       "  'hampton',\n",
       "  'ortega',\n",
       "  'patton',\n",
       "  'swanson',\n",
       "  'joseph',\n",
       "  'francis',\n",
       "  'goodman',\n",
       "  'maldonado',\n",
       "  'yates',\n",
       "  'becker',\n",
       "  'erickson',\n",
       "  'hodges',\n",
       "  'rios',\n",
       "  'conner',\n",
       "  'adkins',\n",
       "  'webster',\n",
       "  'norman',\n",
       "  'malone',\n",
       "  'hammond',\n",
       "  'flowers',\n",
       "  'cobb',\n",
       "  'moody',\n",
       "  'quinn',\n",
       "  'blake',\n",
       "  'maxwell',\n",
       "  'pope',\n",
       "  'floyd',\n",
       "  'osborne',\n",
       "  'paul',\n",
       "  'mccarthy',\n",
       "  'guerrero',\n",
       "  'lindsey',\n",
       "  'estrada',\n",
       "  'sandoval',\n",
       "  'gibbs',\n",
       "  'tyler',\n",
       "  'gross',\n",
       "  'fitzgerald',\n",
       "  'stokes',\n",
       "  'doyle',\n",
       "  'sherman',\n",
       "  'saunders',\n",
       "  'wise',\n",
       "  'colon',\n",
       "  'gill',\n",
       "  'alvarado',\n",
       "  'greer',\n",
       "  'padilla',\n",
       "  'simon',\n",
       "  'waters',\n",
       "  'nunez',\n",
       "  'ballard',\n",
       "  'schwartz',\n",
       "  'mcbride',\n",
       "  'houston',\n",
       "  'christensen',\n",
       "  'klein',\n",
       "  'pratt',\n",
       "  'briggs',\n",
       "  'parsons',\n",
       "  'mclaughlin',\n",
       "  'zimmerman',\n",
       "  'french',\n",
       "  'buchanan',\n",
       "  'moran',\n",
       "  'copeland',\n",
       "  'roy',\n",
       "  'pittman',\n",
       "  'brady',\n",
       "  'mccormick',\n",
       "  'holloway',\n",
       "  'brock',\n",
       "  'poole',\n",
       "  'frank',\n",
       "  'logan',\n",
       "  'owen',\n",
       "  'bass',\n",
       "  'marsh',\n",
       "  'drake',\n",
       "  'wong',\n",
       "  'jefferson',\n",
       "  'park',\n",
       "  'morton',\n",
       "  'abbott',\n",
       "  'sparks',\n",
       "  'patrick',\n",
       "  'norton',\n",
       "  'huff',\n",
       "  'clayton',\n",
       "  'massey',\n",
       "  'lloyd',\n",
       "  'figueroa',\n",
       "  'carson',\n",
       "  'bowers',\n",
       "  'roberson',\n",
       "  'barton',\n",
       "  'tran',\n",
       "  'lamb',\n",
       "  'harrington',\n",
       "  'casey',\n",
       "  'boone',\n",
       "  'cortez',\n",
       "  'clarke',\n",
       "  'mathis',\n",
       "  'singleton',\n",
       "  'wilkins',\n",
       "  'cain',\n",
       "  'bryan',\n",
       "  'underwood',\n",
       "  'hogan',\n",
       "  'mckenzie',\n",
       "  'collier',\n",
       "  'luna',\n",
       "  'phelps',\n",
       "  'mcguire',\n",
       "  'allison',\n",
       "  'bridges',\n",
       "  'wilkerson',\n",
       "  'nash',\n",
       "  'summers',\n",
       "  'atkins',\n",
       "  'wilcox',\n",
       "  'pitts',\n",
       "  'conley',\n",
       "  'marquez',\n",
       "  'burnett',\n",
       "  'richard',\n",
       "  'cochran',\n",
       "  'chase',\n",
       "  'davenport',\n",
       "  'hood',\n",
       "  'gates',\n",
       "  'clay',\n",
       "  'ayala',\n",
       "  'sawyer',\n",
       "  'roman',\n",
       "  'vazquez',\n",
       "  'dickerson',\n",
       "  'hodge',\n",
       "  'acosta',\n",
       "  'flynn',\n",
       "  'espinoza',\n",
       "  'nicholson',\n",
       "  'monroe',\n",
       "  'wolf',\n",
       "  'morrow',\n",
       "  'kirk',\n",
       "  'randall',\n",
       "  'anthony',\n",
       "  'whitaker',\n",
       "  'oconnor',\n",
       "  'skinner',\n",
       "  'ware',\n",
       "  'molina',\n",
       "  'kirby',\n",
       "  'huffman',\n",
       "  'bradford',\n",
       "  'charles',\n",
       "  'gilmore',\n",
       "  'dominguez',\n",
       "  'oneal',\n",
       "  'bruce',\n",
       "  'lang',\n",
       "  'combs',\n",
       "  'kramer',\n",
       "  'heath',\n",
       "  'hancock',\n",
       "  'gallagher',\n",
       "  'gaines',\n",
       "  'shaffer',\n",
       "  'short',\n",
       "  'wiggins',\n",
       "  'mathews',\n",
       "  'mcclain',\n",
       "  'fischer',\n",
       "  'wall',\n",
       "  'small',\n",
       "  'melton',\n",
       "  'hensley',\n",
       "  'bond',\n",
       "  'dyer',\n",
       "  'cameron',\n",
       "  'grimes',\n",
       "  'contreras',\n",
       "  'christian',\n",
       "  'wyatt',\n",
       "  'baxter',\n",
       "  'snow',\n",
       "  'mosley',\n",
       "  'shepherd',\n",
       "  'larsen',\n",
       "  'hoover',\n",
       "  'beasley',\n",
       "  'glenn',\n",
       "  'petersen',\n",
       "  'whitehead',\n",
       "  'meyers',\n",
       "  'keith',\n",
       "  'garrison',\n",
       "  'vincent',\n",
       "  'shields',\n",
       "  'horn',\n",
       "  'savage',\n",
       "  'olsen',\n",
       "  'schroeder',\n",
       "  'hartman',\n",
       "  'woodard',\n",
       "  'mueller',\n",
       "  'kemp',\n",
       "  'deleon',\n",
       "  'booth',\n",
       "  'patel',\n",
       "  'calhoun',\n",
       "  'wiley',\n",
       "  'eaton',\n",
       "  'cline',\n",
       "  'navarro',\n",
       "  'harrell',\n",
       "  'lester',\n",
       "  'humphrey',\n",
       "  'parrish',\n",
       "  'duran',\n",
       "  'hutchinson',\n",
       "  'hess',\n",
       "  'dorsey',\n",
       "  'bullock',\n",
       "  'robles',\n",
       "  'beard',\n",
       "  'dalton',\n",
       "  'avila',\n",
       "  'vance',\n",
       "  'rich',\n",
       "  'blackwell',\n",
       "  'york',\n",
       "  'johns',\n",
       "  'blankenship',\n",
       "  'trevino',\n",
       "  'salinas',\n",
       "  'campos',\n",
       "  'pruitt',\n",
       "  'moses',\n",
       "  'callahan',\n",
       "  'golden',\n",
       "  'montoya',\n",
       "  'hardin',\n",
       "  'guerra',\n",
       "  'mcdowell',\n",
       "  'carey',\n",
       "  'stafford',\n",
       "  'gallegos',\n",
       "  'henson',\n",
       "  'wilkinson',\n",
       "  'booker',\n",
       "  'merritt',\n",
       "  'miranda',\n",
       "  'atkinson',\n",
       "  'orr',\n",
       "  'decker',\n",
       "  'hobbs',\n",
       "  'preston',\n",
       "  'tanner',\n",
       "  'knox',\n",
       "  'pacheco',\n",
       "  'stephenson',\n",
       "  'glass',\n",
       "  'rojas',\n",
       "  'serrano',\n",
       "  'marks',\n",
       "  'hickman',\n",
       "  'english',\n",
       "  'sweeney',\n",
       "  'strong',\n",
       "  'prince',\n",
       "  'mcclure',\n",
       "  'conway',\n",
       "  'walter',\n",
       "  'roth',\n",
       "  'maynard',\n",
       "  'farrell',\n",
       "  'lowery',\n",
       "  'hurst',\n",
       "  'nixon',\n",
       "  'weiss',\n",
       "  'trujillo',\n",
       "  'ellison',\n",
       "  'sloan',\n",
       "  'juarez',\n",
       "  'winters',\n",
       "  'mclean',\n",
       "  'randolph',\n",
       "  'leon',\n",
       "  'boyer',\n",
       "  'villarreal',\n",
       "  'mccall',\n",
       "  'gentry',\n",
       "  'carrillo',\n",
       "  'kent',\n",
       "  'ayers',\n",
       "  'lara',\n",
       "  'shannon',\n",
       "  'sexton',\n",
       "  'pace',\n",
       "  'hull',\n",
       "  'leblanc',\n",
       "  'browning',\n",
       "  'velasquez',\n",
       "  'leach',\n",
       "  'chang',\n",
       "  'house',\n",
       "  'sellers',\n",
       "  'herring',\n",
       "  'noble',\n",
       "  'foley',\n",
       "  'bartlett',\n",
       "  'mercado',\n",
       "  'landry',\n",
       "  'durham',\n",
       "  'walls',\n",
       "  'barr',\n",
       "  'mckee',\n",
       "  'bauer',\n",
       "  'rivers',\n",
       "  'everett',\n",
       "  'bradshaw',\n",
       "  'pugh',\n",
       "  'velez',\n",
       "  'rush',\n",
       "  'estes',\n",
       "  'dodson',\n",
       "  'morse',\n",
       "  'sheppard',\n",
       "  'weeks',\n",
       "  'camacho',\n",
       "  'bean',\n",
       "  'barron',\n",
       "  'livingston',\n",
       "  'middleton',\n",
       "  'spears',\n",
       "  'branch',\n",
       "  'blevins',\n",
       "  'chen',\n",
       "  'kerr',\n",
       "  'mcconnell',\n",
       "  'hatfield',\n",
       "  'harding',\n",
       "  'ashley',\n",
       "  'solis',\n",
       "  'herman',\n",
       "  'frost',\n",
       "  'giles',\n",
       "  'blackburn',\n",
       "  'william',\n",
       "  'pennington',\n",
       "  'woodward',\n",
       "  'finley',\n",
       "  'mcintosh',\n",
       "  'koch',\n",
       "  'best',\n",
       "  'solomon',\n",
       "  'mccullough',\n",
       "  'dudley',\n",
       "  'nolan',\n",
       "  'blanchard',\n",
       "  'rivas',\n",
       "  'brennan',\n",
       "  'mejia',\n",
       "  'kane',\n",
       "  'benton',\n",
       "  'joyce',\n",
       "  'buckley',\n",
       "  'haley',\n",
       "  'valentine',\n",
       "  'maddox',\n",
       "  'russo',\n",
       "  'mcknight',\n",
       "  'buck',\n",
       "  'moon',\n",
       "  'mcmillan',\n",
       "  'crosby',\n",
       "  'berg',\n",
       "  'dotson',\n",
       "  'mays',\n",
       "  'roach',\n",
       "  'church',\n",
       "  'chan',\n",
       "  'richmond',\n",
       "  'meadows',\n",
       "  'faulkner',\n",
       "  'oneill',\n",
       "  'knapp',\n",
       "  'kline',\n",
       "  'barry',\n",
       "  'ochoa',\n",
       "  'jacobson',\n",
       "  'gay',\n",
       "  'avery',\n",
       "  'hendricks',\n",
       "  'horne',\n",
       "  'shepard',\n",
       "  'hebert',\n",
       "  'cherry',\n",
       "  'cardenas',\n",
       "  'mcintyre',\n",
       "  'whitney',\n",
       "  'waller',\n",
       "  'holman',\n",
       "  'donaldson',\n",
       "  'cantu',\n",
       "  'terrell',\n",
       "  'morin',\n",
       "  'gillespie',\n",
       "  'fuentes',\n",
       "  'tillman',\n",
       "  'sanford',\n",
       "  'bentley',\n",
       "  'peck',\n",
       "  'key',\n",
       "  'salas',\n",
       "  'rollins',\n",
       "  'gamble',\n",
       "  'dickson',\n",
       "  'battle',\n",
       "  'santana',\n",
       "  'cabrera',\n",
       "  'cervantes',\n",
       "  'howe',\n",
       "  'hinton',\n",
       "  'hurley',\n",
       "  'spence',\n",
       "  'zamora',\n",
       "  'yang',\n",
       "  'mcneil',\n",
       "  'suarez',\n",
       "  'case',\n",
       "  'petty',\n",
       "  'gould',\n",
       "  'mcfarland',\n",
       "  'sampson',\n",
       "  'carver',\n",
       "  'bray',\n",
       "  'rosario',\n",
       "  'macdonald',\n",
       "  'stout',\n",
       "  'hester',\n",
       "  'melendez',\n",
       "  'dillon',\n",
       "  'farley',\n",
       "  'hopper',\n",
       "  'galloway',\n",
       "  'potts',\n",
       "  'bernard',\n",
       "  'joyner',\n",
       "  'stein',\n",
       "  'aguirre',\n",
       "  'osborn',\n",
       "  'mercer',\n",
       "  'bender',\n",
       "  'franco',\n",
       "  'rowland',\n",
       "  'sykes',\n",
       "  'benjamin',\n",
       "  'travis',\n",
       "  'pickett',\n",
       "  'crane',\n",
       "  'sears',\n",
       "  'mayo',\n",
       "  'dunlap',\n",
       "  'hayden',\n",
       "  'wilder',\n",
       "  'mckay',\n",
       "  'coffey',\n",
       "  'mccarty',\n",
       "  'ewing',\n",
       "  'cooley',\n",
       "  'vaughan',\n",
       "  'bonner',\n",
       "  'cotton',\n",
       "  'holder',\n",
       "  'stark',\n",
       "  'ferrell',\n",
       "  'cantrell',\n",
       "  'fulton',\n",
       "  'lynn',\n",
       "  'lott',\n",
       "  'calderon',\n",
       "  'rosa',\n",
       "  'pollard',\n",
       "  'hooper',\n",
       "  'burch',\n",
       "  'mullen',\n",
       "  'fry',\n",
       "  'riddle',\n",
       "  'levy',\n",
       "  'david',\n",
       "  'duke',\n",
       "  'odonnell',\n",
       "  'guy',\n",
       "  'michael',\n",
       "  'britt',\n",
       "  'frederick',\n",
       "  'daugherty',\n",
       "  'berger',\n",
       "  'dillard',\n",
       "  'alston',\n",
       "  'jarvis',\n",
       "  'frye',\n",
       "  'riggs',\n",
       "  'chaney',\n",
       "  'odom',\n",
       "  'duffy',\n",
       "  'fitzpatrick',\n",
       "  'valenzuela',\n",
       "  'merrill',\n",
       "  'mayer',\n",
       "  'alford',\n",
       "  'mcpherson',\n",
       "  'acevedo',\n",
       "  'donovan',\n",
       "  'barrera',\n",
       "  'albert',\n",
       "  'cote',\n",
       "  'reilly',\n",
       "  'compton',\n",
       "  'raymond',\n",
       "  'mooney',\n",
       "  'mcgowan',\n",
       "  'craft',\n",
       "  'cleveland',\n",
       "  'clemons',\n",
       "  'wynn',\n",
       "  'nielsen',\n",
       "  'baird',\n",
       "  'stanton',\n",
       "  'snider',\n",
       "  'rosales',\n",
       "  'bright',\n",
       "  'witt',\n",
       "  'stuart',\n",
       "  'hays',\n",
       "  'holden',\n",
       "  'rutledge',\n",
       "  'kinney',\n",
       "  'clements',\n",
       "  'castaneda',\n",
       "  'slater',\n",
       "  'hahn',\n",
       "  'emerson',\n",
       "  'conrad',\n",
       "  'burks',\n",
       "  'delaney',\n",
       "  'pate',\n",
       "  'lancaster',\n",
       "  'sweet',\n",
       "  'justice',\n",
       "  'tyson',\n",
       "  'sharpe',\n",
       "  'whitfield',\n",
       "  'talley',\n",
       "  'macias',\n",
       "  'irwin',\n",
       "  'burris',\n",
       "  'ratliff',\n",
       "  'mccray',\n",
       "  'madden',\n",
       "  'kaufman',\n",
       "  'beach',\n",
       "  'goff',\n",
       "  'cash',\n",
       "  'bolton',\n",
       "  'mcfadden',\n",
       "  'levine',\n",
       "  'good',\n",
       "  'byers',\n",
       "  'kirkland',\n",
       "  'kidd',\n",
       "  'workman',\n",
       "  'carney',\n",
       "  'dale',\n",
       "  'mcleod',\n",
       "  'holcomb',\n",
       "  'england',\n",
       "  'finch',\n",
       "  'head',\n",
       "  'burt',\n",
       "  'hendrix',\n",
       "  'sosa',\n",
       "  'haney',\n",
       "  'franks',\n",
       "  'sargent',\n",
       "  'nieves',\n",
       "  'downs',\n",
       "  'rasmussen',\n",
       "  'bird',\n",
       "  'hewitt',\n",
       "  'lindsay',\n",
       "  'le',\n",
       "  'foreman',\n",
       "  'valencia',\n",
       "  'oneil',\n",
       "  'delacruz',\n",
       "  'vinson',\n",
       "  'dejesus',\n",
       "  'hyde',\n",
       "  'forbes',\n",
       "  'gilliam',\n",
       "  'guthrie',\n",
       "  'wooten',\n",
       "  'huber',\n",
       "  'barlow',\n",
       "  'boyle',\n",
       "  'mcmahon',\n",
       "  'buckner',\n",
       "  'rocha',\n",
       "  ...]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words =  stop_word_gen()\n",
    "stop_words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5346bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting without stopwords\n",
    "\n",
    "def preprocess(data, join=False, cont=False, lem=False ):\n",
    "    d = preprocess_text(data, forcount = cont)\n",
    "    d = tokenizer(d)[1]\n",
    "    \n",
    "    if cont:\n",
    "        e=[]\n",
    "        for word in d:\n",
    "            if word.isalpha():\n",
    "                e.append(word.lower())\n",
    "            elif word == '.' :\n",
    "                e.append(word)\n",
    "            \n",
    "    else:\n",
    "        e = [word.lower() for word in d if word.isalpha()]\n",
    "    for i in stop_words:    \n",
    "        e = [word for word in e if word not in i]\n",
    "        \n",
    "        # Join the filtered words back into a string\n",
    "        #filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    if lem:\n",
    "        e = lemmatized(e)\n",
    "    if join:\n",
    "        e = ' '.join(e)\n",
    "\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55da688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syllable find\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def syllables(data):\n",
    "    ST = SyllableTokenizer()\n",
    "    data = lemmatized(data)\n",
    "    syll = [ST.tokenize(token) for token in data]\n",
    "    count=0\n",
    "    total_syl=0\n",
    "    for i in syll:\n",
    "        if len(i)>2:\n",
    "            count+=1            \n",
    "            \n",
    "        if len(i)>0:\n",
    "            total_syl+=len(i)            \n",
    "    return count, total_syl\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1178112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=syllables(preprocess(df.iloc[0].text))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cf375",
   "metadata": {},
   "source": [
    "# TEXTUAL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9796d5",
   "metadata": {},
   "source": [
    "## Countings and pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7cb6194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drang\\anaconda3\\envs\\CV0\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'ï'\n",
      "  warnings.warn(\n",
      "C:\\Users\\drang\\anaconda3\\envs\\CV0\\lib\\site-packages\\nltk\\tokenize\\sonority_sequencing.py:102: UserWarning: Character not defined in sonority_hierarchy, assigning as vowel: 'é'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#complex words variable\n",
    "df[\"COMPLEX WORD COUNT\"]= pd.DataFrame(df['text']).applymap(lambda x:syllables(preprocess(str(x)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38219bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pronouns(text):\n",
    "  pronoun_count = re.compile(r'\\b(I|we|ours|my|mine|(?-i:us))\\b', re.I)\n",
    "  pronouns = pronoun_count.findall(text)\n",
    "  #print(pronouns)\n",
    "  return len(pronouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a339e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#personal pronouns before applying stop_words\n",
    "df[\"PERSONAL PRONOUNS\"]= pd.DataFrame(df['text']).applymap(lambda x:count_pronouns(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49ec2521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>text</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>\\nIntroduction\\n“If anything kills over 10 mil...</td>\n",
       "      <td>436</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>\\nHuman minds, a fascination in itself carryin...</td>\n",
       "      <td>224</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>\\nIntroduction\\nAI is rapidly evolving in the ...</td>\n",
       "      <td>415</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>\\n“Anything that could give rise to smarter-th...</td>\n",
       "      <td>241</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>\\n“Machine intelligence is the last invention ...</td>\n",
       "      <td>325</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>\\nReconciling with the financial realities of ...</td>\n",
       "      <td>160</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>\\nWhat Is an Investment?\\nAn investment is a r...</td>\n",
       "      <td>293</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>\\nQuality and affordable healthcare is a visio...</td>\n",
       "      <td>259</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>\\nAnalytics is a statistical scientific proces...</td>\n",
       "      <td>199</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>\\nBig Data\\nTo begin with I shall first like t...</td>\n",
       "      <td>215</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "                                                  text  COMPLEX WORD COUNT  \\\n",
       "0    \\nIntroduction\\n“If anything kills over 10 mil...                 436   \n",
       "1    \\nHuman minds, a fascination in itself carryin...                 224   \n",
       "2    \\nIntroduction\\nAI is rapidly evolving in the ...                 415   \n",
       "3    \\n“Anything that could give rise to smarter-th...                 241   \n",
       "4    \\n“Machine intelligence is the last invention ...                 325   \n",
       "..                                                 ...                 ...   \n",
       "109  \\nReconciling with the financial realities of ...                 160   \n",
       "110  \\nWhat Is an Investment?\\nAn investment is a r...                 293   \n",
       "111  \\nQuality and affordable healthcare is a visio...                 259   \n",
       "112  \\nAnalytics is a statistical scientific proces...                 199   \n",
       "113  \\nBig Data\\nTo begin with I shall first like t...                 215   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  PERSONAL PRONOUNS  \n",
       "0                                  13                  1  \n",
       "1                                   8                  7  \n",
       "2                                  10                  3  \n",
       "3                                   8                 17  \n",
       "4                                  10                 16  \n",
       "..                                ...                ...  \n",
       "109                                 8                  9  \n",
       "110                                11                  2  \n",
       "111                                 9                  2  \n",
       "112                                12                  0  \n",
       "113                                 8                  8  \n",
       "\n",
       "[114 rows x 6 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a257eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\3024775734.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"AVG NUMBER OF WORDS PER SENTENCE\"].iloc[i] = int(len(wt)/len(st))\n"
     ]
    }
   ],
   "source": [
    "#Average Number of Words Per Sentence\n",
    "df.insert(column=\"AVG NUMBER OF WORDS PER SENTENCE\",loc=4, value=0)\n",
    " \n",
    "for i in range(df.shape[0]):\n",
    "    try:\n",
    "        z=preprocess(df.iloc[i].text, join=True, cont=True)\n",
    "        st = sent_tokenize(z)\n",
    "        wt = word_tokenize(z)\n",
    "    except:\n",
    "        st=[1]\n",
    "        wt=[]\n",
    "    df[\"AVG NUMBER OF WORDS PER SENTENCE\"].iloc[i] = int(len(wt)/len(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef7cb6",
   "metadata": {},
   "source": [
    "## Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2858e6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\171468037.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"AVG SENTENCE LENGTH\"].iloc[i] = int(len(wt)/len(st))\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\171468037.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"PERCENTAGE OF COMPLEX WORDS\"].iloc[i]  = df[\"COMPLEX WORD COUNT\"].iloc[i] / words\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\171468037.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"FOG INDEX\"].iloc[i]  = 0.4 * (df[\"AVG SENTENCE LENGTH\"].iloc[i] + df[\"PERCENTAGE OF COMPLEX WORDS\"].iloc[i])\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\171468037.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"WORD COUNT\"].iloc[i] = words\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\171468037.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"AVG WORD LENGTH\"].iloc[i] = int(c/words)\n"
     ]
    }
   ],
   "source": [
    "#Analysis of Readability\n",
    "\n",
    "df.insert(column=\"AVG SENTENCE LENGTH\",loc=6, value=0)\n",
    "df.insert(column=\"PERCENTAGE OF COMPLEX WORDS\", loc=7, value=0)\n",
    "df.insert(column=\"FOG INDEX\", loc=8, value=0)\n",
    "df.insert(column=\"WORD COUNT\", loc=9, value=0)\n",
    "df.insert(column=\"AVG WORD LENGTH\", loc=5, value=0)\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    c=0\n",
    "    try:\n",
    "        z=preprocess(df.iloc[i].text, join=True, cont=True) \n",
    "        zt = preprocess(df.iloc[i].text, join=True, cont=False) # counts without dot as entity \n",
    "        st = sent_tokenize(z)\n",
    "        #print(st)\n",
    "        wt = word_tokenize(zt)\n",
    "        words=len(wt)\n",
    "        for w in wt:\n",
    "            c+=len(w)\n",
    "    except:\n",
    "        st=[1]\n",
    "        wt=[]\n",
    "        words = 1\n",
    "        c=0\n",
    "    #print(words)\n",
    "    df[\"AVG SENTENCE LENGTH\"].iloc[i] = int(len(wt)/len(st))\n",
    "    df[\"PERCENTAGE OF COMPLEX WORDS\"].iloc[i]  = df[\"COMPLEX WORD COUNT\"].iloc[i] / words\n",
    "    df[\"FOG INDEX\"].iloc[i]  = 0.4 * (df[\"AVG SENTENCE LENGTH\"].iloc[i] + df[\"PERCENTAGE OF COMPLEX WORDS\"].iloc[i])\n",
    "    \n",
    "    df[\"WORD COUNT\"].iloc[i] = words\n",
    "    df[\"AVG WORD LENGTH\"].iloc[i] = int(c/words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae5fb4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL_ID                                                                             45\n",
       "URL                                 https://insights.blackcoffer.com/how-machine-l...\n",
       "text                                \\nMachine learning techniques may have been us...\n",
       "COMPLEX WORD COUNT                                                                112\n",
       "AVG NUMBER OF WORDS PER SENTENCE                                                    9\n",
       "AVG WORD LENGTH                                                                     6\n",
       "PERSONAL PRONOUNS                                                                   2\n",
       "AVG SENTENCE LENGTH                                                                 8\n",
       "PERCENTAGE OF COMPLEX WORDS                                                  0.377104\n",
       "FOG INDEX                                                                    3.350842\n",
       "WORD COUNT                                                                        297\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fb12b",
   "metadata": {},
   "source": [
    "## Syllables Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b603e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\4105373461.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"SYLLABLE PER WORD\"].iloc[i] = count\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.insert(column=\"SYLLABLE PER WORD\", loc=10, value=0)\n",
    "vowels = 'aeiou'\n",
    "worded=[]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    count=0\n",
    "\n",
    "    try:\n",
    "        for base_word in preprocess(df.iloc[i].text):\n",
    "                \n",
    "                base_words = lemmatizer.lemmatize(base_word, pos='v')\n",
    "    \n",
    "                if base_words.endswith(\"ed\"):\n",
    "                    base_words = base_words[:-2]\n",
    "                elif base_words.endswith(\"es\"):\n",
    "                    base_words = base_words[:-2]\n",
    "                elif base_words.endswith(\"s\"):\n",
    "                    base_words = base_words[:-1]\n",
    "                l = re.findall(f'[{vowels}]', base_words, re.I)\n",
    "                count+=len(l)\n",
    "    except:\n",
    "        count=0\n",
    "    df[\"SYLLABLE PER WORD\"].iloc[i] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1c85714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom syllable counter with more specific of counting vowels\n",
    "df.iloc[1]['SYLLABLE PER WORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18d4a42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1266"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting of syllables according to nltk syllables library\n",
    "a,b=syllables(preprocess(df.iloc[1].text))\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d845d0f1",
   "metadata": {},
   "source": [
    "## Derived Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25dce2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inputnlp.xlsx', 'negative-words.txt', 'positive-words.txt']\n",
      "[]\n",
      "['StopWords_Auditor.txt', 'StopWords_Currencies.txt', 'StopWords_DatesandNumbers.txt', 'StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Geographic.txt', 'StopWords_Names.txt']\n"
     ]
    }
   ],
   "source": [
    "#Extracting Derived variables\n",
    "at=[]\n",
    "pat = r\"C:\\Users\\drang\\crawl\\New folder\"\n",
    "for (root,dirs,files) in os.walk(pat, topdown=True):\n",
    "    print(files)\n",
    "    at.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d2ff6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inputnlp.xlsx', 'negative-words.txt', 'positive-words.txt']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc343e",
   "metadata": {},
   "source": [
    "## creating dictionary of pos and neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e053ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading master dictionary of pos and neg words\n",
    "\n",
    "dicts = dict()\n",
    "with open(pat+ \"\\\\\"+ at[0][2], encoding='latin-1') as ot:\n",
    "    a = ot.read()\n",
    "    a = preprocess(a)\n",
    "    dicts[\"positive\"] = a\n",
    "with open(pat+ \"\\\\\"+ at[0][1], encoding='latin-1') as ot:\n",
    "    a = ot.read()\n",
    "    a = preprocess(a)\n",
    "    dicts[\"negative\"] = a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "14e359d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_derives(words):\n",
    "    p=0\n",
    "    n=0\n",
    "    for word in words:\n",
    "        if word in dicts['positive']:\n",
    "            #print(word)\n",
    "            p += 1\n",
    "        \n",
    "        elif word in dicts['negative']:\n",
    "            n += 1\n",
    "        \n",
    "        else:\n",
    "            continue\n",
    "    return p, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc621ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\3347722324.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"POSITIVE SCORE\"].iloc[i] = p\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\3347722324.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"NEGATIVE SCORE\"].iloc[i] = -n\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\3347722324.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"POLARITY SCORE\"].iloc[i] = ps\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\3347722324.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"SUBJECTIVITY SCORE\"].iloc[i] = ss\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\3347722324.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"POSITIVE SCORE\"].iloc[i] = 0\n",
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\3347722324.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"NEGATIVE SCORE\"].iloc[i] = 0\n"
     ]
    }
   ],
   "source": [
    "df.insert(column=\"POSITIVE SCORE\", loc=10, value=0)\n",
    "df.insert(column=\"NEGATIVE SCORE\", loc=11, value=0)\n",
    "df.insert(column=\"POLARITY SCORE\", loc=12, value=0)\n",
    "df.insert(column=\"SUBJECTIVITY SCORE\", loc=13, value=0)\n",
    "for i in range(df.shape[0]):\n",
    "    try:\n",
    "        p,n = count_derives(preprocess(df.iloc[i].text, lem=False)) # BETTER WITHOUT LEMMEATION\n",
    "        ps=(p - n)/((p + n) + 0.000001)\n",
    "        ss=(p + n)/((df.iloc[i]['WORD COUNT']) + 0.000001)\n",
    "\n",
    "        df[\"POSITIVE SCORE\"].iloc[i] = p\n",
    "        df[\"NEGATIVE SCORE\"].iloc[i] = -n\n",
    "        df[\"POLARITY SCORE\"].iloc[i] = ps\n",
    "        df[\"SUBJECTIVITY SCORE\"].iloc[i] = ss\n",
    "    except:\n",
    "        df[\"POSITIVE SCORE\"].iloc[i] = 0\n",
    "        df[\"NEGATIVE SCORE\"].iloc[i] = 0\n",
    "        df[\"POLARITY SCORE\"].iloc[i] = 0\n",
    "        df[\"SUBJECTIVITY SCORE\"].iloc[i] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b04a0",
   "metadata": {},
   "source": [
    "## Saving as excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc1b326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_excel(\"Output Data Structure1.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "759026f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"Average Number of Words Per Sentence\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b5a6da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['URL_ID', 'URL', 'text', 'COMPLEX WORD COUNT',\n",
       "       'AVG NUMBER OF WORDS PER SENTENCE', 'AVG WORD LENGTH',\n",
       "       'PERSONAL PRONOUNS', 'AVG SENTENCE LENGTH',\n",
       "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'POSITIVE SCORE',\n",
       "       'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
       "       'SYLLABLE PER WORD', 'WORD COUNT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb8c90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"Output Data Structure.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "388d8c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drang\\AppData\\Local\\Temp\\ipykernel_17136\\935174234.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2[j].iloc[i] = df[j].iloc[i]\n"
     ]
    }
   ],
   "source": [
    "#converting df to df2\n",
    "for i in range(df2.shape[0]):\n",
    "    for j in df2.columns:\n",
    "        df2[j].iloc[i] = df[j].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4aa5d66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.102285</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.474429</td>\n",
       "      <td>4.989771</td>\n",
       "      <td>13.0</td>\n",
       "      <td>436.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>0.221053</td>\n",
       "      <td>0.181644</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.428298</td>\n",
       "      <td>2.971319</td>\n",
       "      <td>8.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>1377.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.125313</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.520050</td>\n",
       "      <td>3.808020</td>\n",
       "      <td>10.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>2282.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.151565</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.397035</td>\n",
       "      <td>2.958814</td>\n",
       "      <td>8.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>0.373494</td>\n",
       "      <td>0.114799</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.449516</td>\n",
       "      <td>3.779806</td>\n",
       "      <td>10.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-0.106383</td>\n",
       "      <td>0.122396</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>3.366667</td>\n",
       "      <td>8.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.073209</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.456386</td>\n",
       "      <td>4.182555</td>\n",
       "      <td>11.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-0.232877</td>\n",
       "      <td>0.130590</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.463327</td>\n",
       "      <td>3.385331</td>\n",
       "      <td>9.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.108635</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.554318</td>\n",
       "      <td>4.621727</td>\n",
       "      <td>12.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>-0.088235</td>\n",
       "      <td>0.141962</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.448852</td>\n",
       "      <td>2.979541</td>\n",
       "      <td>8.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "112     149  https://insights.blackcoffer.com/business-anal...   \n",
       "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              62.0           -32.0        0.319149            0.102285   \n",
       "1              58.0           -37.0        0.221053            0.181644   \n",
       "2              65.0           -35.0        0.300000            0.125313   \n",
       "3              64.0           -28.0        0.391304            0.151565   \n",
       "4              57.0           -26.0        0.373494            0.114799   \n",
       "..              ...             ...             ...                 ...   \n",
       "109            21.0           -26.0       -0.106383            0.122396   \n",
       "110            35.0           -12.0        0.489362            0.073209   \n",
       "111            28.0           -45.0       -0.232877            0.130590   \n",
       "112            35.0            -4.0        0.794872            0.108635   \n",
       "113            31.0           -37.0       -0.088235            0.141962   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                   12.0                     0.474429   4.989771   \n",
       "1                    7.0                     0.428298   2.971319   \n",
       "2                    9.0                     0.520050   3.808020   \n",
       "3                    7.0                     0.397035   2.958814   \n",
       "4                    9.0                     0.449516   3.779806   \n",
       "..                   ...                          ...        ...   \n",
       "109                  8.0                     0.416667   3.366667   \n",
       "110                 10.0                     0.456386   4.182555   \n",
       "111                  8.0                     0.463327   3.385331   \n",
       "112                 11.0                     0.554318   4.621727   \n",
       "113                  7.0                     0.448852   2.979541   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                                13.0               436.0       919.0   \n",
       "1                                 8.0               224.0       523.0   \n",
       "2                                10.0               415.0       798.0   \n",
       "3                                 8.0               241.0       607.0   \n",
       "4                                10.0               325.0       723.0   \n",
       "..                                ...                 ...         ...   \n",
       "109                               8.0               160.0       384.0   \n",
       "110                              11.0               293.0       642.0   \n",
       "111                               9.0               259.0       559.0   \n",
       "112                              12.0               199.0       359.0   \n",
       "113                               8.0               215.0       479.0   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0               2544.0                1.0              7.0  \n",
       "1               1377.0                7.0              7.0  \n",
       "2               2282.0                3.0              7.0  \n",
       "3               1614.0               17.0              7.0  \n",
       "4               1945.0               16.0              7.0  \n",
       "..                 ...                ...              ...  \n",
       "109             1000.0                9.0              7.0  \n",
       "110             1716.0                2.0              7.0  \n",
       "111             1475.0                2.0              7.0  \n",
       "112             1045.0                0.0              8.0  \n",
       "113             1274.0                8.0              7.0  \n",
       "\n",
       "[114 rows x 15 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f9a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
